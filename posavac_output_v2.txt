Program Evaluation
An Overview


Steps to measure the success of the Sexual Assault Prevention Program:
1. Collect feedback from participants on the program’s effectiveness and usefulness.
2. Gather data on changes in attitudes and behaviors related to sexual assault before, during, and after the program.
3. Assess the impact of the program on campus-wide knowledge about sexual assault and resources for victims.
4. Analyze the cost-effectiveness of the program compared to other initiatives.
5. Monitor changes in reported incidents of sexual assault on campus.
6. Track the number of participants who complete the program and their satisfaction ratings.
7. Review any relevant research studies or surveys that are conducted on the program. 
8. Gauge the level of engagement among participants in the program’s activities. 
9. Evaluate the program’s capacity to reach

 Program evaluation is the process of assessing a program to determine its strengths and weaknesses, so that resources can be allocated more effectively. This evaluation is important when considering programs meant to reduce sexual assaults on campus, because it allows staff to determine if the program is meeting the needs of students.

 Program evaluation is a way to assess services in modern society, using research methods and concepts from various fields. It helps to identify needs and determine whether the service meets these needs at an acceptable cost without negative side effects.


Organizational program evaluation is harder than self-evaluation because it involves a team of people with shared responsibility for the results, and those results are not the sole responsibility of any one person.

 Programs that take longer to achieve desired outcomes are harder to evaluate since there is no clear criteria for success, and finding solutions to any problems encountered can be difficult.

 Evaluating programs involves a variety of people, and if evaluators do not have the trust of individual staff members, it can be threatening for them because their livelihoods could be negatively impacted.

 Programs are often funded by entities other than the clients of the program, and this can lead to job security being based on satisfying the funders rather than providing quality services to clients.

 Program evaluation is a form of evaluation that focuses on discovering the value and worth of programs in human services such as education, medical care, welfare, rehabilitation, and job training. It is more difficult to do than evaluating our own activities because it involves different focuses, purposes, and methodologies.

EVALUATION AREAS THAT NEED TO BE CONSIDERED

Program evaluation is a much broader set of activities than just measuring the outcome of a program, and good evaluators must consider seven general areas - needs assessment, design, implementation, data collection, data analysis, interpretation, and reporting. Choosing not to include one or more elements should be a deliberate decision based on a clear rationale.

Meeting Needs
 Program evaluations are conducted on existing programs and often do not address the original need that led to the program. These evaluations may also fail to document important aspects of the need assessment component of the evaluation.

 Evaluators focus on unmet needs when assessing the effectiveness of existing programs or developing new ones. Unmet needs can be considered from a variety of perspectives, such as how they are expected to change in the future or how they are perceived differently by different people.

Implementation


An implementation evaluation is a type of project that focuses on confirming that program personnel have been hired, offices have been opened, and clients have received the services they need. It is different from an outcome evaluation, which also considers the results of interventions.

 Program evaluation is important to monitor the implementation of a program, as it is common for programs to not be put into practice or have discrepancies between what is intended and what is actually happening. Estimates show that failure rates for implementing changes can range from 28-93%. Evaluators are necessary to monitor the details of how the program has been implemented.

Stakeholders

Evaluators should take into account the many people involved with a program, including those who provide and receive it and those who are supportive or opposed. All of these people have a stake in the program and are referred to as "stakeholders."


Staff providing services are highly involved in the evaluation process, and must be taken into account. This includes therapists, administrators, support staff, board members, families of clients, and people in the community affected by the program. Additionally, those living near the treatment facility may have reactions to details such as traffic or client behavior.

 Evaluators should make reasonable efforts to learn about the major stakeholders and any important current issues in order to be successful in their evaluations. It is impossible to make all stakeholders happy, but it is essential to understand their different expectations and implications for the progress of the evaluation.

 Evaluation researchers have explored principles for successful collaboration with stakeholders, such as clarifying the motivation for collaboration, and have found both benefits and liabilities associated with stakeholder involvement in evaluation.
Side Effects


Side effects of problem solving efforts can be both positive and negative. Medications can have minor or serious side effects, while welfare policies may create long-term dependency. In the educational and social service realm, special education classes can have a stigma attached to them, but they can also help children with unique needs learn at a comfortable pace. Unforeseen side effects are sometimes positive, such as increased self-esteem when people learn new skills. Program planners must seek to provide services that benefit people while minimizing any negative side effects.

Improvement Focus
 The area of improvement focus involves considering how a program can become better in the future and is especially important when an evaluation occurs early in a program's life. Formative evaluations explicitly look for ways to improve the program, while even summative evaluations can benefit from an improvement focus.

 Evaluators must be honest and forthright in their evaluations, but they should also focus on how to improve a program even if it has substantial shortcomings or problems.

Outcomes


An evaluation of a program should consider the results, such as how clients improve or how those involved in the program have changed because of their participation. The proof of the program is seen in those who have gone through it, and expected outcomes vary depending on the particular program.

 Evaluators examine outcomes and the evidence that those outcomes are caused by the program. They look at how clients have improved, if their perceptions of their skills have increased, and if the program is responsible for the changes seen.


When outcomes are clear, the task may not be finished. There are multiple ways to compare aspects of the outcomes that are important in some evaluations such as cost, value to stakeholders, or time it takes to achieve a given result. Evaluators must consider these ways in order to determine the overall success of a program.
Nuances (Mechanisms)
 Evaluations are increasingly aimed at examining more sophisticated questions about why good outcomes occur, such as what mechanisms were involved and how different groups or conditions experience better or worse results. It is important to consider whether certain services and activities are necessary for achieving good results, and if not, they could be eliminated. Additionally, it is important to understand the different effects of treatment across different age groups in order to determine which treatments are most effective.


When evaluating mechanisms in an evaluation, it is important to include measures of the suspected mechanisms. This may include assessing how much a client successfully restructured thoughts, or a broader measure such as the degree to which the planned intervention was followed. Data analysis techniques can then be used to analyze these measures and determine if they had an effect on outcomes.
MISSION


The MISSION acronym (Meeting Needs, Implementation, Stakeholders, Side Effects, Improvement, Outcomes, and Nuances) is a useful tool for evaluators to remember the seven elements of a good evaluation. It is possible to minimize or even ignore some of these areas as needed, but the MISSION acronym should be used to help students remember each element and how it applies to the evaluation.
COMMON TYPES OF PROGRAM EVALUATIONS

Program evaluation involves various types of studies, including need, process, outcome, and efficiency, to meet its primary goals.
Assess Needs of the Program Participants
 Need evaluation is an important first step in program planning, as it helps to identify and measure unmet needs. Program evaluation occurs before the program starts, in order to determine which alternatives are best suited for meeting those needs.
 Evaluators assess need by examining the socioeconomic profile, social problems, and current services in a community. They also look at residents' and local leaders' opinions to see which programs are likely to be accepted and which are not, and if there is a need for self-determination or empowerment.
Examine the Process of Meeting the Needs

Evaluations of process involve checking the assumptions made during the planning stage, assessing the needs of those served, and verifying that the program activities match the planned ones. This helps to ensure that the program is operating as expected and can be replicated with other populations or locations.
Measure the Outcomes and Impacts of a Program



An evaluation of a program's outcomes can measure whether participants are doing well and possess the skills taught, compare performance between those in the program and those not receiving its services, and show that receiving services caused a change for the better.



Program managers hope that their programs can cause positive changes in people, but it is difficult to identify the causes of behavioral changes. Experienced evaluators must balance gathering information and providing services while conducting evaluations.


Evaluators assessing the outcome of a program often find that people have different opinions about what constitutes success. An evaluator must take into account all perspectives and not adopt just one view of the desired outcome.

 It can be difficult to evaluate the success of a drug rehabilitation program due to the influence of peers, and the difficulty of changing long-standing behaviors. Positive outcomes may be observed after a person's participation in the program, but these changes can disappear quickly. To accurately assess the success of a program, it is necessary to look at both short-term outcomes and long-term impacts.

Integrate the Needs, Costs, and Outcomes

 Evaluators must take cost into account when determining the success of a program. Cost-effectiveness analysis can be used to compare programs with similar outcomes, while evaluations with high level integration of multiple elements can help administrators make choices between programs with different outcomes.

 Evaluations should be conducted in a logical sequence, from need to efficiency, in order to maximize their value.

IMPORTANT ISSUES IN EVALUATIONS

Time Frames of Needs

Short Term Needs

 There are programs to help people in crises, such as medical care, emotional support, and financial assistance. Educational services are also available to meet specific needs of employees, and should be available on short notice.

Long-Term Needs
 Services must be available for a long period of time to meet certain needs, such as education, psychotherapy, treatment for chronic illnesses, training for prison inmates, and rehabilitation for alcoholism and drug abuse.

Potential Needs

Ideally, potential problems can be prevented through preventive programs like immunization, health education, or business security. Evaluations of prevention programs must differ from evaluations of programs designed to address existing problems.
Extensiveness of Programs


Program evaluations vary depending on the type of program, size of group, and level of implementation. Although certain evaluation tools may be used for all levels of evaluation, the focus and scale of the evaluation will differ depending on the individual needs of the program and the decisions faced by administrators.

This text discusses the evaluation of smaller programs typically encountered by evaluators working in school districts, hospitals, personnel departments, social service agencies and governments. While some national programs are mentioned, most focus on local government or educational programs that vary depending on implementation at a local agency. Evaluators from these organizations are expected to submit program evaluations to the funding source.
 Evaluators can often improve data summaries and presentations for local organizations so that program information can be used to answer questions faced by program administrators, without needing complicated statistical procedures.
PURPOSE OF PROGRAM EVALUATION
 The overall purpose of program evaluation activities is to contribute to the provision of quality services to people in need by providing feedback from program activities and outcomes to those who can make changes or decide which services are offered.
 Program evaluation serves as a feedback loop to provide information to develop program improvements or make wise choices between programs. This is done by assessing needs, measuring implementation, evaluating goals and objectives, and comparing costs of the program to similar programs.


 Feedback can be given for formative or summative purposes. Formative evaluation is used to help form the program while summative evaluation helps decide whether the program should be started, continued, or chosen from two or more alternatives. Evaluation findings may not always lead to specific decisions based on the evaluation.


Monitoring is an important form of evaluation used to maintain the quality of a program after it has been successfully implemented. It can also serve a formative or summative purpose if changes in the community or economic conditions require it.


Evaluations are used to learn about specific programs, but they are not usually used to gain a deeper understanding of them. Evaluators recognize the need for further understanding of programs and interventions, but this text does not focus on such types of evaluation because they involve more complex tasks than most evaluators typically handle.
THE ROLES OF EVALUATORS
Principles, Competencies, and Credentialing
 Evaluators should adhere to standards or principles, possess certain competencies, and be credentialed in order to produce good evaluations.
 The American Evaluation Association and the Canadian Evaluation Society both have adopted standards and principles to guide and support evaluators in their work. These are more aspirational than precise, but provide a clear intent for evaluators to strive for.
 Proposals for a list of competencies for evaluators have been around for over a decade and are seen favorably in the field. The Canadian Evaluation Society has formally adopted these competencies, while the American Evaluation Association has not done so yet. However, many evaluators still demonstrate their voluntary commitment to these values.
 The American Evaluation Association (AEA) offers the Certified Evaluation Specialist (CES) designation to members who meet its criteria. There is also discussion of credentialing and accreditation processes for evaluators and evaluation programs in the United States, but the trade-offs between quality and flexibility have not yet been resolved.

Comparison of Internal and External Evaluators
 Evaluators can be either internal (working for the organization needing evaluation services) or external (working for a research firm, university, or government agency). The affiliation of an evaluator impacts how evaluations are conducted.
Factors Related to Competence
 Internal evaluators have an advantage since they have better access to program directors and to the administrators of the organization, which gives them more insight into how the program is running. This makes it easier for them to ask relevant questions during the planning and interpretation of evaluations.


The technical expertise of an evaluator is important, and external evaluators typically have access to a wider variety of resources. However, some independent evaluators work alone and may not have as much access to specialized resources.
 Organizations can avoid errors due to inexperience by hiring external evaluators with experience in the type of program being evaluated.
Personal Quauties


An evaluator's technical competence is important for an effective evaluation, but personal qualities such as objectivity and fairness are also essential. Internal evaluators can often gain more trust from program directors and staff than external evaluators, which increases the likelihood that they can do their job properly. Evaluators must be careful to avoid allowing personal biases to affect their conclusions in order to maintain credibility.

Internal evaluators can be expected to have a commitment to improving the organization that pays their salary, which could affect their objectivity. External evaluators are less likely to experience conflicting pressures when an evaluation reveals problems in a program. Evaluators should remember that most deficiencies in programs are due to system problems rather than personal inadequacies.

Factors Related to the Purpose oe an Evaluation
 Internal evaluators may have an advantage in performing formative and quality assurance evaluations, while external evaluators are better suited for summative evaluations. This is because summative evaluations can have a greater impact on decision-making and may be more difficult for an internal evaluator to perform without bias or causing dissatisfaction among residents.
 Internal and external evaluators both have strengths and weaknesses which can vary based on the evaluation. It is possible to form a partnership between them to capitalize on the strengths of both.
Evaluation and Service


Program evaluators bridge the gap between social scientists and practitioners, using research methods to collect and analyze data while remaining sensitive to the needs of both service delivery staff and administrators. Evaluators must be prepared to navigate different priorities and styles in order to successfully carry out their role.
 Participating in a new field has its advantages and disadvantages. The advantages include intellectual stimulation, satisfaction of seeing research methods used to benefit people, and the ability to talk about what works well. The disadvantages include being viewed as intrusive and unnecessary by some service delivery personnel, and having to ask challenging questions and tactfully insist that answers be supported by data.
 When evaluation is built into program management and all stakeholders agree that program improvement is both possible and desirable, potential conflicts between evaluation, service delivery, and administration may be minimized.

Activities Often Confused with Program Evaluation
 Program evaluation is distinct from basic research, individual assessment, and compliance audits. Confusing them can make evaluation more difficult.


Basic research is concerned with theoretical questions, while program evaluation focuses on providing information to help people and organizations improve their effectiveness in the short-term. Evaluation helps inform program decisions, but it should not be used primarily as a means of contributing to the development of theories. Cooperation may be lost if program staff feel that the evaluation is primarily for research purposes.
 Human service staff members often confuse program evaluation with assessments done by educational and personnel psychologists to determine a person's need for service or qualifications for a job. Program evaluation does not diagnose people, determine eligibility for benefits, or choose whom to hire or promote. Instead, its purpose is to learn how well a program is helping people improve on relevant variables.
 Program evaluators and program auditors examine government-sponsored programs for different objectives. Program auditors look to ensure the funds are being spent as Congress intended, while program evaluators are interested in how the services have impacted the people they are intended to serve.
 Program auditors and evaluators are related, but their roles have changed in recent years. The change of the GAO from "General Accounting Office" to "Government Accountability Office" reflects this change, but there is still potential for misunderstanding if an evaluator is viewed as an auditor.
Evaluation and Related Activities of Organizations
 Evaluation is sometimes combined with research, education and staff development, auditing, planning, and human resources.

Research

Human service organizations may sponsor research through grants or operating funds, which can provide valuable insight to evaluators. For example, a police department was awarded a grant to study the effect of providing emotional support to victims of crimes. Researchers in universities, hospitals and other research institutions working on similar issues can be valuable colleagues for an evaluator.
Education and State Development
 Educational organizations have combined evaluation and education for a long time, such as evaluating study materials or monitoring educational programs. They also use it for faculty development and employee training.
Auditing
 Some states have combined the functions of program evaluation and program oversight often carried out by state auditors. Coordinating these efforts can be done through inspector general offices in the federal and state governments.
Pianning

 Evaluators with program evaluation skills are needed for planning activities to estimate the level of community acceptance, as they can construct surveys and analyze the data obtained.

CASE STUDY 1
Open Arms
 REACH Evaluation presented a comprehensive program evaluation report of the Open Arms Children’s Health Dental Service at Home of the Innocents, which was funded by the Social Innovation Fund and in partnership with the University of Louisville School of Dentistry's Pediatric Dental Program.
Meeting Needs
 The "Documentation of Need" section of the Open Arms grant application was a three-page summary of research that highlighted the need for dental care among foster children and supported it with evidence demonstrating the prevalence of poor oral health in Kentucky.
Implementation
 The report addressed the issue of what was done in the program by providing copies of medical notes and detailing the progression of staffing strategies.
Stakeholders

The evaluation report considered the perspectives of multiple stakeholders, including clients and their family members, permanent staff, and dental students and residents who received training.
Side Effects


The report found that providing dental services on site saved staff time in several ways, including reducing the need to transport children to dental services and allowing for more efficient scheduling of visits. No negative side effects were noted.
Improvement Focus

Evaluators requested and listed suggestions from staff and clients for improving the program, and also suggested strategies to address the challenge of continuing it after ending its initial outside funding.
Outcomes


The program evaluation report showed that 920 and 1,411 children were served over two 12-month periods respectively, 1,475 special needs individuals were served during the 21-month period, and the planned number of students and residents was met. Statistical analyses indicated a significant improvement in treatment acceptance over time with greater improvement for some groups than others. Qualitative data also highlighted improvements in medical notes.

SUMMARY AND PREVIEW
 Program evaluation is used to assess unmet needs, document implementation, look for both positive and negative side effects, measure outcomes, and learn from them in order to improve programs. Evaluators help agencies by assessing program value and making corrections.
 Chapter 2 discusses how to plan an evaluation by working with program staff and sponsors to ease any fears they may have about the process.
STUDY QUESTIONS


Examples of activities related to program evaluation from newspapers or news magazines include: 
- Medical care: A review of existing hospital policies and procedures to determine their effectiveness.
- Education: An analysis of school funding allocations to see if they are meeting the needs of students.
- Public policy: An assessment of current economic stimulus programs to see if they are having a positive impact on the economy.


The validity of such comments depends on the specific social programs that are being discussed. Some programs may be seen as symbolic efforts and may not have measurable impacts on social problems, while other programs may be effective in tackling certain issues. The reports from the first question can provide insights into which programs are actually making a difference.
 Program evaluation activities can be used in organizations to assess the effectiveness of a program, identify areas for improvement, and inform decisions about program continuation or modification. In a familiar organization, these activities could include surveys, focus groups, interviews, data analysis, and other methods. 

Program evaluation activities can be applied in an organization with which I am familiar in order to assess the effectiveness of a program, identify areas for improvement, and inform decisions about program continuation or modification. For example, if the organization is rolling out a new customer service initiative, they could use program evaluation activities such as surveys, focus groups, interviews, and data analysis to measure how successful it has been so far.

Surveys can be used to get feedback from customers on the quality of customer service they have received from the organization. This could involve asking customers questions about their experience interacting with the organization, their satisfaction with

Advantages:
1. Helps ensure that programs are achieving desired outcomes.
2. Allows for informed decision making about program design and implementation.
3. Helps to identify areas of improvement and successes.
4. Can provide valuable feedback from program participants.

Disadvantages:
1. Can be expensive and time consuming.
2. Can be difficult to accurately measure outcomes.
3. Results can be hard to interpret. 
4. Can lead to program “tunnel vision” if only certain outcomes are measured.


The ideas on evaluating the Sexual Assault Prevention Program can vary depending on each student's personal beliefs and values. Different students may prioritize different outcomes, such as reducing sexual assault incidents, changing attitudes and behaviors, or increasing reporting of sexual assault cases. The reasons behind these different choices could be based on their own experiences, knowledge of the issue, or what they believe to be the best way to address the problem.
ADDITIONAL RESOURCE
W. K. Kellogg Foundation. (2010). Evaluation handbook. Retrieved from .
 This free resource provides a helpful overview and succinct details of program evaluation processes, including how values and assumptions affect evaluation work.



